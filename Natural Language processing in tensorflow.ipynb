{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\faizalam\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\faizalam\\anaconda3\\lib\\site-packages (from h5py) (1.18.1)\n",
      "Requirement already satisfied: six in c:\\users\\faizalam\\anaconda3\\lib\\site-packages (from h5py) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 371369\n",
      "all_embeddings dimensions: (371369, 300)\n",
      "/c/de/auftr√§gen\n"
     ]
    }
   ],
   "source": [
    "# Load the file and pull out words and embeddings\n",
    "import h5py\n",
    "\n",
    "with h5py.File('datasets/mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {0}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(all_embeddings.shape))\n",
    "\n",
    "print(all_words[1337])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 166967\n",
      "all_embeddings dimensions: (166967, 300)\n",
      "acoustical\n"
     ]
    }
   ],
   "source": [
    "# restrict our vocablury to just english words\n",
    "english_words=[word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices=[i for i,word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings=all_embeddings[english_word_indices]\n",
    "print(\"all_words dimensions: {0}\".format(len(english_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(english_embeddings.shape))\n",
    "\n",
    "print(english_words[1337])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "norms=np.linalg.norm(english_embeddings,axis=1)\n",
    "normalized_embeddings=english_embeddings.astype('float32')/norms.astype('float32').reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index={word: i for i,word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0\n",
      "cat\tfeline\t 0.81091654\n",
      "cat\tdog\t 0.58677226\n",
      "cat\tkitty\t 0.819126\n",
      "mouse\tmouse\t 0.99999994\n",
      "antonyms\topposites\t 0.31266534\n",
      "antonyms\tsynonyms\t 0.48834023\n",
      "love\thate\t 1.0\n"
     ]
    }
   ],
   "source": [
    "#measuring similarity between words\n",
    "def similarity_score(w1,w2):\n",
    "  score = np.dot(normalized_embeddings[index[w1],:],normalized_embeddings[index[w2],:])\n",
    "  return score  \n",
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tkitty\\t', similarity_score('cat', 'kitty'))\n",
    "print('mouse\\tmouse\\t', similarity_score('mouse', 'mouse'))\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))                                                                       \n",
    "print('love\\thate\\t', similarity_score('love', 'love'))                                                                                                                                              \n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding most similar words to a given words\n",
    "def closest_to_vector(v,n):\n",
    "  all_scores = np.dot(normalized_embeddings,v)\n",
    "  best_words = map(lambda i:english_words[i], reversed(np.argsort(all_scores)))\n",
    "  return [next(best_words) for _ in range(n)]\n",
    "\n",
    "def most_similar(w,n):\n",
    "   return closest_to_vector(normalized_embeddings[index[w],:],n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'humane_society', 'kitten', 'kitty', 'cats', 'feline', 'colocolo', 'housecat', 'maine_coon', 'moggie']\n",
      "['dog', 'dogs', 'doggy_paddle', 'good_friend', 'lhasa_apso', 'wire_haired_dachshund', 'cadaver_dog', 'woof_woof', 'golden_retrievers', 'scenthound']\n",
      "['mahal', 'taj', 'taj_mahal', 'dhivehi', 'mumtaz', 'udaipur', 'jahan', 'serai', 'rajasthan', 'raj']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('cat', 10))\n",
    "print(most_similar('dog', 10))\n",
    "print(most_similar('mahal', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sister']\n",
      "['wife']\n",
      "['paris']\n"
     ]
    }
   ],
   "source": [
    "# solve_analogy\n",
    "def solve_analogy(a1,b1,a2):\n",
    "    b2=normalized_embeddings[index[b1], :]-normalized_embeddings[index[a1],:]+normalized_embeddings[index[a2], : ]\n",
    "    return closest_to_vector(b2,1)\n",
    "\n",
    "print(solve_analogy(\"man\",\"brother\",\"woman\"))\n",
    "print(solve_analogy(\"man\", \"husband\", \"woman\"))\n",
    "print(solve_analogy(\"britain\", \"london\", \"france\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using WOrd EMbeddings in deep models and sentimental analysis approach\n",
    "\n",
    "import string\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y=int(line[0])\n",
    "    \n",
    "     # Split the line into words using Python's split() function\n",
    "    words=line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    \n",
    " \n",
    "\n",
    "   # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "with open(\"movie-simple.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    dataset=[convert_line_to_example(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size=100\n",
    "total_batches=len(dataset)//batch_size\n",
    "train_batches=3*total_batches//4\n",
    "train,test=dataset[:train_batches*batch_size],dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001C18723FB08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001C18723FB08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001C187033308>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001C187033308>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001C187033108>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001C187033108>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "#building mlp \n",
    "import tensorflow as tf\n",
    "\n",
    "#Supress warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#placeholder for input\n",
    "x=tf.placeholder(tf.float32,[None,300])\n",
    "y=tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "#three layer MLP\n",
    "h1=tf.layers.dense(x,100,tf.nn.relu)\n",
    "h2=tf.layers.dense(h1,20,tf.nn.relu)\n",
    "logits=tf.layers.dense(h2,1)\n",
    "probabilities=tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=y))\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.round(probabilities),y),tf.float32))\n",
    "\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "init_op= tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.6899133920669556 \t Acc: 0.5600000023841858\n",
      "Epoch: 10 \t Loss: 0.6865106821060181 \t Acc: 0.5199999809265137\n",
      "Epoch: 20 \t Loss: 0.6806683540344238 \t Acc: 0.5099999904632568\n",
      "Epoch: 30 \t Loss: 0.6754550337791443 \t Acc: 0.5299999713897705\n",
      "Epoch: 40 \t Loss: 0.6513552665710449 \t Acc: 0.6000000238418579\n",
      "Epoch: 50 \t Loss: 0.6276317834854126 \t Acc: 0.6499999761581421\n",
      "Epoch: 60 \t Loss: 0.5802499055862427 \t Acc: 0.8100000023841858\n",
      "Epoch: 70 \t Loss: 0.542470395565033 \t Acc: 0.8100000023841858\n",
      "Epoch: 80 \t Loss: 0.4524689018726349 \t Acc: 0.8799999952316284\n",
      "Epoch: 90 \t Loss: 0.42233946919441223 \t Acc: 0.8999999761581421\n",
      "Epoch: 100 \t Loss: 0.32607659697532654 \t Acc: 0.949999988079071\n",
      "Epoch: 110 \t Loss: 0.27644532918930054 \t Acc: 0.9200000166893005\n",
      "Epoch: 120 \t Loss: 0.20447510480880737 \t Acc: 0.9599999785423279\n",
      "Epoch: 130 \t Loss: 0.25389957427978516 \t Acc: 0.9300000071525574\n",
      "Epoch: 140 \t Loss: 0.15890401601791382 \t Acc: 0.9800000190734863\n",
      "Epoch: 150 \t Loss: 0.2250901758670807 \t Acc: 0.9100000262260437\n",
      "Epoch: 160 \t Loss: 0.17043569684028625 \t Acc: 0.949999988079071\n",
      "Epoch: 170 \t Loss: 0.12364044040441513 \t Acc: 0.9700000286102295\n",
      "Epoch: 180 \t Loss: 0.11490640044212341 \t Acc: 0.9800000190734863\n",
      "Epoch: 190 \t Loss: 0.12671221792697906 \t Acc: 0.9599999785423279\n",
      "Epoch: 200 \t Loss: 0.19890977442264557 \t Acc: 0.9300000071525574\n",
      "Epoch: 210 \t Loss: 0.09419842064380646 \t Acc: 0.9700000286102295\n",
      "Epoch: 220 \t Loss: 0.1682896465063095 \t Acc: 0.9200000166893005\n",
      "Epoch: 230 \t Loss: 0.0901680439710617 \t Acc: 0.9700000286102295\n",
      "Epoch: 240 \t Loss: 0.091086246073246 \t Acc: 0.9800000190734863\n",
      "Final accuracy: 0.9610705375671387\n"
     ]
    }
   ],
   "source": [
    "init_op= tf.global_variables_initializer()\n",
    "#Train\n",
    "sess=tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(250):\n",
    "    for batch in range(train_batches):\n",
    "        data=train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews=[sample['x'] for sample in data]\n",
    "        labels  =[sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1,1])\n",
    "        _, l, acc=sess.run([train_step,loss,accuracy],feed_dict={x:reviews,y:labels})\n",
    "    if epoch%10 ==0:\n",
    "        print(\"Epoch: {0} \\t Loss: {1} \\t Acc: {2}\".format(epoch, l, acc))\n",
    "        \n",
    "        \n",
    "    random.shuffle(train)\n",
    "    \n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels  = np.array(test_labels).reshape([-1, 1])\n",
    "\n",
    "acc = sess.run(accuracy, feed_dict={x: test_reviews, y: test_labels})\n",
    "print(\"Final accuracy: {0}\".format(acc))    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exciting [[0.9998894]]\n",
      "hated [[0.]]\n",
      "boring [[8.817332e-08]]\n",
      "loved [[0.99999917]]\n",
      "enjoyable [[0.9990632]]\n",
      "disgusting [[9.870593e-08]]\n"
     ]
    }
   ],
   "source": [
    "#check our sentiment analysis model\n",
    "words_to_test=[\"exciting\",\"hated\",\"boring\",\"loved\",\"enjoyable\",\"disgusting\"]\n",
    "\n",
    "\n",
    "for word in words_to_test:\n",
    "    print(word,sess.run(probabilities,feed_dict={x:normalized_embeddings[index[word]].reshape(1,300)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
